import platform
import sys
import pysam
from pathlib import Path
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import os

from datetime import datetime
from dimelo import parse_bam, plot_reads, load_processed, plot_read_browser
import h5py


def system_info():
    """Print system information."""
    print('System:', platform.system())
    print('Release:', platform.release())
    print('Version:', platform.version())
    print('Processor:', platform.processor())
    print('Python version:', sys.version)

def get_reference_sequence(ref_genome_file,  region_chr, region_start, region_end):
    """Fetch reference sequence from genome file."""
    if not ref_genome_file.exists():
        print(f"Reference genome file not found: {ref_genome_file}")
        sys.exit(1)
    try:
        ref_seq = pysam.FastaFile(ref_genome_file).fetch(region_chr, region_start-1, region_end-1) # removing 1 from the coord indexes s the reference genome starts with 1 and the pysam.FastaFile is 0-based and the coordinates are the index-1
        # ref_seq = pysam.FastaFile(ref_genome_file).fetch(region_chr, region_start, region_end) # removing 1 from the coord indexes s the reference genome starts with 1 and the pysam.FastaFile is 0-based and the coordinates are the index-1
        ref_seq_list = list(ref_seq)
        print(ref_seq)
        print(len(ref_seq))
        return ref_seq_list
    except Exception as e:
        print("Error fetching reference sequence:", e)
        return None

def create_output_directory(path):
    """Create output directory if it doesn't exist."""
    try:
        output_dir = Path(path)
        output_dir.mkdir(exist_ok=True)
        return output_dir
    except Exception as e:
        print("Error creating output directory:", e)
        return None

def extract_from_bam(experiment_name, bam_path, ref_genome_file, output_dir, window_size=None, threshold_mC=0.99, 
                    num_cores=32, regions='chr1:206586162-206586192', motifs=['CG,0'], 
                    output_name='extract_output', save_fig=True):
    """Processes a BAM file using parse_bam.extract and plots the extracted reads."""
    try:
        # Parse regions to calculate region length
        region_chr, region_coords = regions.split(':')
        region_start, region_end = map(int, region_coords.split('-'))
        region_length = region_end - region_start
        print(f"Region length: {region_length}")

        extract_file, extract_regions = parse_bam.extract(
            input_file=bam_path,
            output_name=output_name,
            ref_genome=ref_genome_file,
            output_directory=output_dir,
            regions=regions,
            motifs=motifs,
            thresh=threshold_mC,
            window_size=window_size,
        )

        if threshold_mC == None: 
            fig_plot_browser = plot_read_browser.plot_read_browser(
                mod_file_name=extract_file,# mod_file_name: str | Path,
                region=regions, #region: str,
                motifs=motifs, #motifs: list[str],
                thresh=threshold_mC, # thresh: int | float | None = None,
                single_strand = False, # : bool = False,
                sort_by=['shuffle', 'strand'], #: str | list[str] = "shuffle",
                hover = True, #: bool = True,
                )
            fig_plot_browser.update_layout(  
                title=f"{experiment_name}<br>Extracted Reads for {regions}",
            )
            # fig.show()
            if save_fig:
                output_html_path = Path(output_dir) / f"plot_browser_{region_length}bps_{experiment_name}_extract_reads_{regions}.html"
                fig_plot_browser.write_html(str(output_html_path))
                print(f"Plot browser html figure saved to {output_html_path}")
            return extract_file, extract_regions, fig_plot_browser
        else: 
            plot_reads.plot_reads(
                extract_file,
                regions,
                motifs=motifs,
                window_size=window_size,
                sort_by=['shuffle', 'strand'],
                s=1
            )
            plt.xlabel(f'bp relative to {regions}')
            plt.title(f"{experiment_name}<bp>Extracted Reads for {regions}")
            plt.show()
        return extract_file, extract_regions
    
    except Exception as e:
        print("Error in BAM extraction:", e)
        return None, None

def process_extracted_reads_no_fully_unmethylated(extract_file, regions, motifs, ref_seq_list):
    """
    Process extracted reads into a DataFrame.

    Warning: make sure that the ref_seq_list was created using the same region and reference genome using the function: 
        motifs=['CG,0']
        ref_seq_list = get_reference_sequence(ref_genome_v1_1_file, region_chr, region_start, region_end)  
    """
    try:
        reads, read_names, mods, regions_dict = load_processed.readwise_binary_modification_arrays(
            file=extract_file,
            regions=regions,
            motifs=motifs
        )
        reads_df = pd.DataFrame({
            'read_name': read_names,
            'mod': mods,
            'pos': reads
        }).explode('pos')

        # reads_df['pos_shifted'] = reads_df['pos'] + 15
        region_length = len(ref_seq_list)
        reads_df['pos_shifted'] = reads_df['pos'] + (region_length // 2)
        return reads_df, regions_dict
    except Exception as e:
        print("Error processing extracted reads:", e)
        return None, None


def process_extracted_reads_add_fully_unmethylated(extract_file, regions, motifs, ref_seq_list):
    """
    Process extracted reads into a DataFrame, ensuring all reads (methylated and unmethylated) are included.
    """
    try:
        # Extract methylation-modified positions
        mod_coords, read_ids, mods, regions_dict = load_processed.readwise_binary_modification_arrays(
            file=extract_file,
            regions=regions,
            motifs=motifs
        )

        # Get all read names (both methylated and unmethylated)
        with h5py.File(extract_file, "r") as h5:
            all_read_names = np.array(h5["read_name"], dtype=str)  # Extract all read names

        # Convert read IDs to strings to avoid type mismatches
        read_ids = np.array(read_ids, dtype=str)

        # Create a DataFrame for methylated reads
        reads_df = pd.DataFrame({
            'read_name': read_ids,
            'mod': mods,
            'pos': mod_coords
        })

        # Ensure 'pos' is converted to numeric type
        reads_df['pos'] = pd.to_numeric(reads_df['pos'], errors='coerce')

        # Identify unmethylated reads (present in BAM but missing from reads_df)
        methylated_reads = set(reads_df['read_name'])
        unmethylated_reads = [read for read in all_read_names if read not in methylated_reads]

        # Create a DataFrame for unmethylated reads (no positions)
        unmethylated_df = pd.DataFrame({
            'read_name': unmethylated_reads,
            'mod': None,
            'pos': np.nan  # No methylation site
        })

        # Combine both DataFrames
        reads_df = pd.concat([reads_df, unmethylated_df], ignore_index=True)

        # Ensure 'pos' is numeric for all rows
        reads_df['pos'] = pd.to_numeric(reads_df['pos'], errors='coerce')

        # Compute shifted positions
        region_length = len(ref_seq_list)
        reads_df['pos_shifted'] = reads_df['pos'].apply(
            lambda x: int(x + (region_length // 2)) if not np.isnan(x) else -1
        )

        return reads_df, regions_dict

    except Exception as e:
        print("Error processing extracted reads:", e)
        return None, None

# def has_no_NONE_alignment_scores(row):
#     """
#     Check if all alignment scores for a read are not None.

#     Parameters
#     ----------
#     row : pandas.Series or dict
#         A row from a DataFrame representing a read, expected to contain an 'alignment_scores' field
#         which is a list or array of scores for each base in the region.

#     Output
#     ------
#     bool
#         Returns True if all alignment scores are not None, otherwise False.

#     Goal
#     ----
#     To filter and identify reads that have complete alignment information (no missing scores)
#     across the entire region of interest.
#     """
#     scores = row.get('alignment_scores')
#     if scores is None:
#         return False
#     # Check all scores are not None
#     return all(score is not None for score in scores)


# def count_alignment_scores(row):
#     """
#     Count the total number of alignment_scores that are None in the row.

#     Parameters
#     ----------
#     row : pandas.Series or dict
#         A row from a DataFrame representing a read, expected to contain an 'alignment_scores' field
#         which is a list or array of scores for each base in the region.

#     Output
#     ------
#     int
#         Returns the count of alignment scores that are None.

#     Goal
#     ----
#     To quantify the number of missing alignment scores for a read across the region of interest.
#     """
#     print(row)
#     scores = row.get('alignment_scores')
#     print('scores:', scores)
#     if scores is None:
#         return 0
#     return sum(score is None for score in scores)


def process_extracted_reads(extract_file, regions, motifs, ref_seq_list, keep_full_coverage_reads_only=True):
    """
    Process extracted reads into a DataFrame, ensuring only reads that cover the full start to end DNA coordinates are included.
    This function filters out reads that don't span the complete region of interest.
    
    Parameters:
    -----------
    extract_file : str or Path
        Path to the HDF5 file containing extracted read data
    regions : str
        Genomic region in format 'chr:start-end' (e.g., 'chr1:206586162-206586192')
    motifs : list
        List of motifs to search for (e.g., ['CG,0'])
    ref_seq_list : list
        Reference sequence as a list of nucleotides
    keep_full_coverage_reads_only : bool, optional
        If True, only keeps reads that cover the full start to end DNA coordinates. If False, keeps all reads.
        Default is True.
        
    Returns:
    --------
    tuple : (DataFrame, dict)
        - DataFrame with filtered reads containing columns: read_name, mod, pos, pos_shifted
        - Dictionary containing region information
        
    Example:
    --------
    # Extract reads that cover the full region
    reads_df, regions_dict = process_extracted_reads(
        extract_file='path/to/extracted_reads.h5',
        regions='chr1:206586162-206586192',
        motifs=['CG,0'],
        ref_seq_list=['A', 'T', 'G', 'C', ...],
        keep_full_coverage_reads_only=True
    )
    """
    # try:
    # Extract methylation-modified positions
    mod_coords, read_ids, mods, regions_dict = load_processed.readwise_binary_modification_arrays(
        file=extract_file,
        regions=regions,
        motifs=motifs
    )

    # Get all read names and their coordinate information
    read_starts = None
    read_ends = None
    read_coords = None
    
    with h5py.File(extract_file, "r") as h5:
        all_read_names = np.array(h5["read_name"], dtype=str)  # Extract all read names
        
        # Print the keys in the .h5 file
        # print("Available keys in HDF5 file:", list(h5.keys()))
        
        # Try to get read coordinate information if available
        read_coords_available = False
        try:
            # Check if read coordinates are available in the HDF5 file
            if "read_start" in h5.keys() and "read_end" in h5.keys():
                read_starts = np.array(h5["read_start"])
                read_ends = np.array(h5["read_end"])
                read_coords_available = True
                print(f"Found read coordinates: {len(read_starts)} reads")
            elif "read_coords" in h5.keys():
                read_coords = np.array(h5["read_coords"])
                read_coords_available = True
                print(f"Found read_coords: {len(read_coords)} reads")
            else:
                print("Warning: Read coordinate information not found in HDF5 file.")
        except Exception as e:
            print(f"Warning: Could not access read coordinate information: {e}")

    # Parse the region to get start and end coordinates
    region_chr, region_coords = regions.split(':')
    region_start, region_end = map(int, region_coords.split('-'))
    region_length = region_end - region_start

    # Convert read IDs to actual read names using the all_read_names array
    # read_ids are indices into the all_read_names array
    methylated_read_names = []
    for read_id in read_ids:
        try:
            read_index = int(read_id)
            if read_index < len(all_read_names):
                methylated_read_names.append(all_read_names[read_index])
            else:
                print(f"Warning: Read ID {read_id} is out of bounds for all_read_names array")
                methylated_read_names.append(f"unknown_{read_id}")
        except (ValueError, TypeError):
            print(f"Warning: Could not convert read_id {read_id} to integer")
            methylated_read_names.append(f"unknown_{read_id}")

    # Convert read IDs to strings to avoid type mismatches
    read_ids = np.array(read_ids, dtype=str)

    # Create a DataFrame for methylated reads
    reads_df = pd.DataFrame({
        'read_name_str': methylated_read_names,
        'read_name': read_ids,
        'read_id_number': read_ids,
        'mod': mods,
        'pos': mod_coords
    })
    
    print(f"Unique read names with methylation: {len(reads_df['read_name'].unique())}")

    # Filter reads based on coverage criteria
    if keep_full_coverage_reads_only:
        if read_coords_available:
            # Method 1: Use explicit read coordinates if available
            if read_starts is not None and read_ends is not None:
                # Create a mapping of read names to their coordinates
                read_coord_map = {}
                for i, read_name in enumerate(all_read_names):
                    read_coord_map[read_name] = (read_starts[i], read_ends[i])
                
                # Filter reads that cover the full region
                full_coverage_reads = []
                for read_name in all_read_names:
                    if read_name in read_coord_map:
                        read_start, read_end = read_coord_map[read_name]
                        # Check if read covers the full region (read starts before or at region start and ends after or at region end)
                        if read_start <= region_start and read_end >= region_end:
                            full_coverage_reads.append(read_name)
                
                # Check overlap between full coverage reads and reads with methylation data
                reads_with_methylation = set(reads_df['read_name_str'].unique())
                full_coverage_set = set(full_coverage_reads)
                overlap = reads_with_methylation.intersection(full_coverage_set)
                
                print(f"Found {len(full_coverage_reads)} reads with full coverage")
                print(f"Reads with methylation data: {len(reads_with_methylation)}")
                print(f"Overlap between full coverage and methylation: {len(overlap)}")
                
                # Filter the reads_df to only include full coverage reads
                reads_df = reads_df[reads_df['read_name_str'].isin(full_coverage_reads)]
                print(f"After full coverage filtering: {len(np.unique(reads_df['read_name_str']))} reads with methylation data")
               
                # NEED TO REMOVE READS THAT ARE ONLY PARTLY ALIGNED TO THE REGION.. 
                # Removing reads that have bases with None alignment scores 
                # count_alignment_scores_df = reads_df.apply(count_alignment_scores, axis=1)
                # print(f"Count of None alignment scores per read:\n{count_alignment_scores_df.value_counts()}")
                # print(f"Reads with less than 100 None alignment scores: {(count_alignment_scores_df < 100).sum()}")
                # print("Reads with 100 or more None alignment scores")
                # reads_df = reads_df[reads_df.apply(count_alignment_scores, axis=1) < 100]
                # # reads_df = reads_df[reads_df.apply(has_no_NONE_alignment_scores, axis=1)]
                # print(f"After removing reads that have bases with None alignment scores and full coverage filtering: {len(np.unique(reads_df['read_name_str']))} reads with methylation data")
                

            elif read_coords is not None:
                # Method 2: Use read_coords if available (assuming it contains start/end pairs)
                full_coverage_reads = []
                for i, read_name in enumerate(all_read_names):
                    if i < len(read_coords):
                        read_start, read_end = read_coords[i]
                        if read_start <= region_start and read_end >= region_end:
                            full_coverage_reads.append(read_name)
                
                reads_df = reads_df[reads_df['read_name_str'].isin(full_coverage_reads)]
                print(f"Found {len(full_coverage_reads)} reads with full coverage")
                print(f"After full coverage filtering: {len(np.unique(reads_df['read_name_str']))} reads with methylation data")
                
                # # Removing reads that have bases with None alignment scores 
                # reads_df = reads_df[reads_df.apply(has_no_NONE_alignment_scores, axis=1)]
                # print(f"After removing reads that have bases with None alignment scores and full coverage filtering: {len(np.unique(reads_df['read_name_str']))} reads with methylation data")
                

        else:
            print("Warning: No coordinate information available for full coverage filtering. Keeping all reads with methylation information.")
    
    # # Debug: Show the mapping between read IDs and actual read names
    # print(f"Converted {len(read_ids)} read IDs to actual read names")
    # print(f"Sample mapping: read_id 0 -> {read_names[0] if len(read_names) > 0 else 'N/A'}")
    # Ensure 'pos' is numeric for all rows
    reads_df['pos'] = pd.to_numeric(reads_df['pos'], errors='coerce')

    # Compute shifted positions
    region_length = len(ref_seq_list)
    reads_df['pos_shifted'] = reads_df['pos'].apply(
        lambda x: int(x + (region_length // 2)) if not np.isnan(x) else -1
    )

    print(f"Final result: {len(reads_df)} reads with methylation information out of {len(all_read_names)} total reads")
    return reads_df, regions_dict

    # except Exception as e:
    #     print("Error processing extracted reads with full coverage filtering:", e)
    #     return None, None


def remove_low_methylated_reads(reads_df, threshold_percent=50):
    """
    Remove reads that have less than a specified percentage of the maximum number 
    of methylated CGs per read in the dataset.
    
    Parameters:
    -----------
    reads_df : pandas.DataFrame
        DataFrame containing read methylation data with columns: read_name, mod, pos, pos_shifted
    regions_dict : dict
        Dictionary containing region information
    threshold_percent : float, optional
        Percentage threshold relative to the mean number of methylated CGs per read.
        Reads with fewer methylated CGs than this percentage will be removed.
        Default is 10 (10% of maximum).
        
    Returns:
    --------
    tuple : (DataFrame, dict)
        - Filtered DataFrame with low methylated reads removed
        
    Example:
    --------
    # Remove reads with less than 10% of max methylation
    filtered_reads_df = remove_low_methylated_reads(
        reads_df=reads_df,
        threshold_percent=10
    )
    """
    try:
        # Count methylated CGs per read
        # Convert mod column to numeric to enable mathematical operations
        reads_df['num_CG_methylated'] = pd.to_numeric(reads_df['mod'], errors='coerce').fillna(1)
        # reads_df['num_CG_methylated'] = 1
        
        # Group by read_name and count the number of methylation events (mod=1)
        methylation_counts = reads_df.groupby('read_name')['num_CG_methylated'].sum().reset_index()
        methylation_counts.columns = ['read_name', 'methylation_count']
        
        # # Find the maximum number of methylated CGs across all reads
        max_methylation = methylation_counts['methylation_count'].max()
        # # Find the median number of methylated CGs across all reads
        # max_methylation = methylation_counts['methylation_count'].median()
        # Find the mean number of methylated CGs across all reads
        mean_methylation = methylation_counts['methylation_count'].median()
        
        # Calculate the threshold (10% of maximum by default)
        threshold = mean_methylation * (threshold_percent / 100)
        
        print(f"Mean methylated CGs per read: {mean_methylation}")
        print(f"Maximum methylated CGs per read: {max_methylation}")
        print(f"Threshold ({threshold_percent}% of max): {threshold:.2f}")
        
        # Get reads that meet the threshold
        reads_to_keep = methylation_counts[methylation_counts['methylation_count'] >= threshold]['read_name'].tolist()
        reads_to_remove = methylation_counts[methylation_counts['methylation_count'] < threshold]['read_name'].tolist()
        
        # Filter the original DataFrame
        filtered_reads_df = reads_df[reads_df['read_name'].isin(reads_to_keep)].copy()
        remove_reads_df = reads_df[reads_df['read_name'].isin(reads_to_remove)].copy()
        
        print(f"Original number of reads: {len(reads_df['read_name'].unique())}")
        print(f"Number of reads after filtering: {len(filtered_reads_df['read_name'].unique())}")
        print(f"Removed {len(reads_df['read_name'].unique()) - len(filtered_reads_df['read_name'].unique())} reads")
        
        return filtered_reads_df, methylation_counts, remove_reads_df
        
    except Exception as e:
        print("Error in remove_low_methylated_reads:", e)
        return reads_df

import pysam
from pathlib import Path

def subset_BAM_by_read_IDs(bam_path, remove_reads_df, output_bam_path=None, index_output=True):
    """
    Create a BAM that contains only reads whose names are listed in remove_reads_df['read_name'].

    Parameters
    ----------
    bam_path : str | Path
        Path to the source BAM.
    remove_reads_df : pandas.DataFrame
        DataFrame with a column 'read_name' (the qname in the BAM). Duplicates are OK.
    output_bam_path : str | Path | None
        Where to write the subset BAM. If None, will write alongside the input as
        '<input_basename>.subset_by_remove_reads.bam'.
    index_output : bool
        If True, create a .bai index for the output BAM.

    Returns
    -------
    Path
        Path to the written subset BAM.
    """
    bam_path = Path(bam_path)

    if output_bam_path is None:
        output_bam_path = bam_path.with_suffix("")  # strip .bam
        output_bam_path = Path(str(output_bam_path) + ".subset_by_remove_reads.bam")
    else:
        output_bam_path = Path(output_bam_path)

    # Determine which column has the read names:
    # Prefer 'read_name' (typical). If not present, try a common alternative used upstream.
    if 'read_name_str' in remove_reads_df.columns:
        name_series = remove_reads_df['read_name_str']
    elif 'read_name' in remove_reads_df.columns:
        name_series = remove_reads_df['read_name']
    else:
        raise ValueError(
            "remove_reads_df must contain a 'read_name' or 'read_name_str' column with BAM qnames."
        )

    # Build a set for fast lookups; ensure strings
    target_names = set(map(str, name_series.dropna().unique()))
    if len(target_names) == 0:
        raise ValueError("remove_reads_df has no read names to subset.")

    # Open input and create output with the same header
    with pysam.AlignmentFile(bam_path, "rb") as in_bam:
        with pysam.AlignmentFile(output_bam_path, "wb", header=in_bam.header) as out_bam:
            # Iterate over all records (including unmapped) safely
            for aln in in_bam.fetch(until_eof=True):
                # query_name is the read's QNAME in BAM
                if aln.query_name in target_names:
                    out_bam.write(aln)

    # Optionally index the output BAM
    if index_output:
        # Remove existing index if present to avoid pysam errors
        bai = output_bam_path.with_suffix(output_bam_path.suffix + ".bai")
        if bai.exists():
            bai.unlink()
        pysam.index(str(output_bam_path))

    print(f"Subset BAM written to: {output_bam_path}")
    if index_output:
        print(f"Index written to: {output_bam_path}.bai")

    return output_bam_path



def bam_to_sam(bam_path, sam_path=None):
    # # Example usage:
    # bam_to_sam("input.bam", "output.sam")
    bam_path = Path(bam_path)
    if sam_path is None:
        sam_path = bam_path.with_suffix(".sam")
    else:
        sam_path = Path(sam_path)

    with pysam.AlignmentFile(bam_path, "rb") as bam_file:
        with pysam.AlignmentFile(sam_path, "w", header=bam_file.header) as sam_file:
            for read in bam_file.fetch(until_eof=True):
                sam_file.write(read)

    print(f"Converted BAM → SAM: {sam_path}")
    return sam_path


import pysam
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

def plot_bam_quality_metrics(bam_path):
    # # Example usage
    # plot_bam_quality_metrics("input.bam")

    bam_path = Path(bam_path)

    read_lengths = []
    mapping_qualities = []
    avg_base_qualities = []

    with pysam.AlignmentFile(bam_path, "rb") as bam_file:
        for read in bam_file.fetch(until_eof=True):
            # Skip secondary/supplementary if you only want primary alignments
            if read.is_secondary or read.is_supplementary:
                continue

            read_lengths.append(read.query_length)
            mapping_qualities.append(read.mapping_quality)
            if read.query_qualities is not None:
                avg_base_qualities.append(np.mean(read.query_qualities))
            else:
                avg_base_qualities.append(np.nan)

    # Set plot style
    # sns.set(style="whitegrid", font_scale=1.2)
    sns.set(font_scale=1.2)

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    

    # Read length distribution
    sns.histplot(read_lengths, bins=50, kde=False, ax=axes[0], color="steelblue")
    axes[0].set_title("Read Length Distribution")
    axes[0].set_xlabel("Read Length (bp)")
    axes[0].set_ylabel("Count")

    # Mapping quality distribution
    sns.histplot(mapping_qualities, bins=60, kde=False, ax=axes[1], color="orange")
    axes[1].set_title("Mapping Quality Distribution")
    axes[1].set_xlabel("Mapping Quality")
    axes[1].set_ylabel("Count")

    # Average base quality distribution
    sns.histplot(avg_base_qualities, bins=50, kde=False, ax=axes[2], color="green")
    axes[2].set_title("Average Base Quality per Read")
    axes[2].set_xlabel("Average Phred Quality Score")
    axes[2].set_ylabel("Count")

    # plt.title(f"Run Quality for bam file \n {os.path.basename(bam_path)}\n Total reads processed: {len(read_lengths)}")
    plt.suptitle(f"Run Quality for bam file \n {os.path.basename(bam_path)}\n Total reads processed: {len(read_lengths)}", fontsize=12) 

    fig.subplots_adjust(top=0.88) # Adjust this value as needed

    plt.tight_layout()
    plt.show()

    print(f"Total reads processed: {len(read_lengths)}")



def visualize_data_old(reads_df):
    """Generate visualizations for the data."""
    try:
        reads_df['read_name'].plot(kind='hist', bins=1600, title='#mC of individual Reads Distribution')
        plt.gca().spines[['top', 'right']].set_visible(False)
        plt.show()

        sns.scatterplot(
            data=reads_df,
            x="pos",
            y="read_name",
            hue="mod",
            s=0.5,
            marker="s",
            linewidth=0
        )
        # plt.xticks(ticks=np.arange(len(ref_seq_list)), labels=ref_seq_list, size=font_size) # 'small') #, rotation=90)
        plt.xlabel('Position')
        plt.ylabel('Read Name')
        plt.show()
    except Exception as e:
        print("Error in visualization:", e)

def visualize_data(reads_df):
    """Generate visualizations for the data."""
    try:
        # Ensure 'pos' is numeric and drop NaNs
        reads_df = reads_df.copy()  # Avoid modifying the original DataFrame
        reads_df['pos'] = pd.to_numeric(reads_df['pos'], errors='coerce')
        reads_df = reads_df.dropna(subset=['pos'])  # Remove unmethylated reads for plotting

        # Histogram of read distribution
        reads_df['read_name'].value_counts().plot(kind='bar', title='#mC of individual Reads Distribution')
        plt.gca().spines[['top', 'right']].set_visible(False)
        plt.show()

        # Scatter plot of modifications
        sns.scatterplot(
            data=reads_df,
            x="pos",
            y="read_name",
            hue="mod",
            s=0.5,
            marker="s",
            linewidth=0
        )

        plt.xlabel('Position')
        plt.ylabel('Read Name')
        plt.show()
    except Exception as e:
        print("Error in visualization:", e)

def create_padded_reads_no_fully_unmethylated(reads_df, regions_dict, region_length):
    """Generate padded reads matrix."""
    try:
        read_names_unique = np.unique(reads_df['read_name'])
        num_reads = len(read_names_unique)
        reads_dict = {name: i for i, name in enumerate(read_names_unique)}
        padded_reads = np.full((num_reads, region_length), np.nan)

        for i in range(len(reads_df['read_name'])):
            padded_reads[reads_dict[reads_df['read_name'][i]], reads_df['pos_shifted'][i]] = 1

        return padded_reads
    except Exception as e:
        print("Error creating padded reads matrix:", e)
        return None

def create_padded_reads(reads_df, regions_dict, region_length):
    """Generate padded reads matrix, including reads with no methylation."""
    try:
        # Ensure 'pos_shifted' is numeric
        reads_df['pos_shifted'] = pd.to_numeric(reads_df['pos_shifted'], errors='coerce')

        read_names_unique = np.unique(reads_df['read_name'])
        num_reads = len(read_names_unique)
        reads_dict = {name: i for i, name in enumerate(read_names_unique)}
        padded_reads = np.full((num_reads, region_length), np.nan)

        for _, row in reads_df.iterrows():
            if row['pos_shifted'] >= 0:  # Ignore unmethylated reads (set as -1)
                padded_reads[reads_dict[row['read_name']], int(row['pos_shifted'])] = 1

        return padded_reads

    except Exception as e:
        print("Error creating padded reads matrix:", e)
        return None


def plot_padded_reads(padded_reads, ref_seq_list):
    """Plot padded reads matrix using matshow with x-ticks as reference sequence."""
    try:
        plt.figure(figsize=(10, 150))
        plt.matshow(padded_reads, fignum=1)
        plt.colorbar()
        plt.title("Padded Reads Matrix")

        # Scale font size: decreases as seq_length increases, but within reasonable bounds
        font_size = max(2, min(8, 500 / len(ref_seq_list)))  # Now it stays between 2 and 8

        plt.xticks(ticks=np.arange(len(ref_seq_list)), labels=ref_seq_list, size=font_size) # 'small') #, rotation=90)
        # plt.xlabel("Reference Sequence")

        plt.show()
    except Exception as e:
        print("Error plotting padded reads matrix:", e)


def save_padded_reads(padded_reads, output_dir, file_name):
    """Save padded reads as a NumPy array."""
    try:
        np.save(Path(output_dir, file_name), padded_reads)
        print(f"Padded reads saved to {file_name}")
    except Exception as e:
        print("Error saving padded reads:", e)


# =======================
# For unthresholded data plotting:

def plot_histogram(data, title, num_bins=16):
    # Compute the histogram
    hist, bin_edges = np.histogram(data, bins=num_bins)

    # Create the bar plot
    fig = go.Figure(data=[
        go.Bar(
            x=bin_edges[:-1],  # Start of each bin
            y=hist,            # Frequency in each bin
            width=np.diff(bin_edges),  # Width of each bin
            marker=dict(color='blue', opacity=0.7)
        )
    ])

    # Add labels and title
    fig.update_layout(
        title=title,
        xaxis_title="mod_vector values",
        yaxis_title="Frequency",
        bargap=0.1
    )

    return fig
    

# Define the number of bins
# num_bins = 50
# title=f"Distribution of Non-Zero mod_vector Values<br>Experiment: {experiment_name}<br>Region length: {region_length} [{region_str}]"
# fig_hist = plot_histogram(data=filtered_mod_vector_no0, num_bins=num_bins, title=title)
# # Show the plot
# fig_hist.show()

def plot_mov_values_percentages(filtered_mod_vector_no0, title, num_bins=16):
    # Compute the histogram
    hist, bin_edges = np.histogram(filtered_mod_vector_no0, bins=num_bins)

    # Normalize the histogram to percentages
    percentages = (hist / len(filtered_mod_vector_no0)) * 100

    # Create the bar plot
    fig = go.Figure(data=[
        go.Bar(
            x=bin_edges[:-1],  # Start of each bin
            y=percentages,     # Percentage in each bin
            width=np.diff(bin_edges),  # Width of each bin
            marker=dict(color='blue', opacity=0.7)
        )
    ])

    # Add labels and title
    fig.update_layout(
        title=title,
        xaxis_title="mod_vector values",
        yaxis_title="Percentage",
        bargap=0.1
    )

    # Show the plot
    fig.show()

    # Print the percentage values in each bin
    for i in range(len(percentages)):
        print(f"Bin {i + 1}: Range [{bin_edges[i]:.4f}, {bin_edges[i + 1]:.4f}) - Percentage: {percentages[i]:.2f}%")

# plot_mov_values_percentages(filtered_mod_vector_no0)

def parse_region(region_str):
    # Split the region string into chromosome and range
    region_chr, region_range = region_str.split(':')
    # Split the range into start and end
    region_start, region_end = map(int, region_range.split('-'))
    # Calculate the region length
    region_length = region_end - region_start
    return region_chr, region_start, region_end, region_length

# # Example usage
# region_chr, region_start, region_end, region_length = parse_region(region_str)
# print("region_chr:", region_chr)
# print("region_start:", region_start)
# print("region_end:", region_end)
# print("region_length:", region_length)


def plot_violin_mod_vector(mod_vector, title="Violin Plot of mod_vector Values"):
    """Plot a violin plot of the mod_vector values using seaborn."""
    plt.figure(figsize=(8, 4))
    sns.violinplot(y=mod_vector, inner="box", color="skyblue")
    plt.title(title)
    plt.ylabel("mod_vector value")
    plt.xlabel("")
    plt.tight_layout()
    plt.show()


def mod_vectors_noThreshold_analyze(
    experiment_name,
    extract_file, # bam_path,
    region_str,
    motifs,
    num_bins=16,
    # ref_genome_path,
    # output_dir,
    # threshold_mC=None,
    ):
    # extract_file, extract_regions, fig_plot_browser = extract_from_bam(
    #     experiment_name=experiment_name,
    #     bam_path=bam_path,
    #     ref_genome_file=ref_genome_path,
    #     output_dir=output_dir,
    #     regions=region_str,
    #     motifs=motifs,
    #     output_name='extracted_reads',
    #     threshold_mC=threshold_mC,
    # )
    sorted_read_tuples, readwise_datasets, regions_dict = load_processed.read_vectors_from_hdf5(
                                            extract_file,  # extract_file, #     file: str | Path,
                                            motifs=motifs,  #     motifs: list[str],
                                            regions=region_str,  #     regions: str | Path | list[str | Path] | None = None,
                                            #     window_size: int | None = None,
                                            #     single_strand: bool = False,
                                            #     sort_by: str | list[str] = ["chromosome", "region_start", "read_start"],
                                            #     calculate_mod_fractions: bool = True,
                                        ) # -> tuple[list[tuple], list[str], dict | None]


    # Aggregate the second elements (mod_vector) from all tuples
    aggregated_mod_vector = [read[1] for read in sorted_read_tuples] # np.sum(, axis=0
    # print('aggregated_mod_vector', aggregated_mod_vector)                      

    flattened_mod_vector = np.concatenate(aggregated_mod_vector)
    # print("flattened_mod_vector", flattened_mod_vector)

    # Violin plot of the flattened_mod_vector
    plot_violin_mod_vector(flattened_mod_vector, title=f"Violin Plot of mod_vector Values\nExperiment: {experiment_name} [{region_str}]")

    # Filter out values equal to 0
    filtered_mod_vector_no0 = flattened_mod_vector[flattened_mod_vector != 0]

    # Define the number of bins
    # num_bins = 50
    region_chr, region_start, region_end, region_length = parse_region(region_str)
    title=f"Distribution of Non-Zero mod_vector Values<br>Experiment: {experiment_name}<br>Region length: {region_length} [{region_str}]"
    fig_hist = plot_histogram(data=filtered_mod_vector_no0, num_bins=num_bins, title=title)
    # Show the plot
    fig_hist.show()

    plot_mov_values_percentages(filtered_mod_vector_no0, 
                                title=f"Percentage Distribution of Non-Zero mod_vector Values<br>Experiment: {experiment_name}<br>Region length: {region_length} [{region_str}]",
                                num_bins=num_bins)

    return sorted_read_tuples, readwise_datasets, regions_dict, aggregated_mod_vector, filtered_mod_vector_no0

# sorted_read_tuples, readwise_datasets, regions_dict, aggregated_mod_vector, filtered_mod_vector_no0 = mod_vectors_analyze(
#         experiment_name,
#         extract_file, # bam_path,
#         region_str,
#         motifs
#     )


def main():
    """Main function to execute all tasks."""
    system_info()

    experiment_name = "unedited_T_primerES_nCATS"
    threshold_mC =  0.7 #  0.9 #0.99
    bam_path = "/home/michalula/data/cas9_nanopore/data/20241226_MR_nCATs_TcellsPrES_unedit_P2R9/passed_fast5/5mCG/to_t2t_v1_1/sort_align_trim_20241226_MR_nCATs_TcellsPrES_unedit_P2R9_passed.dna_r9.4.1_e8_sup@v3.3.5mCG.bam"

    date_today = datetime.today().strftime('%Y-%m-%d')

    ref_genome_v1_1_file = Path('/home/michalula/data/ref_genomes/to_t2t_v1_1/chm13.draft_v1.1.fasta')
    reg_genome_version = "t2t_v1_1"
    # t2t_v1_1_cd55_30bps = 'chr1:206586162-206586192'
    region_chr = 'chr1'

    # Expend window size
    expand_window_size = 16 # 50 # 50 #000 
    print("Expend window size by 2 * ", expand_window_size)
    region_start = 206586162 - expand_window_size
    region_end = 206586192 + expand_window_size + 1
    region_str = region_chr + ":" + str(region_start) + "-" + str(region_end) #'chr1:206586162-206586192'
    region_length = region_end - region_start
    print("region_length", region_length)


    save_padded_reads_name_np = f"padded_reads_{experiment_name}_mCthresh{threshold_mC}_{reg_genome_version}_{region_str}_{date_today}.npy"
    output_dir = create_output_directory("./dimelo_v2_output")

    motifs=['CG,0']
    ref_seq_list = get_reference_sequence(ref_genome_v1_1_file, region_chr, region_start, region_end)


    extract_file, extract_regions = extract_from_bam(
        experiment_name=experiment_name,
        bam_path=bam_path,
        ref_genome_file=ref_genome_v1_1_file,
        output_dir=output_dir,
        regions=region_str,
        motifs=motifs,
        output_name='extracted_reads',
        threshold_mC=threshold_mC,
    )

    keep_unmethylated_reads = False
    if extract_file:
        if keep_unmethylated_reads:
            reads_df, regions_dict = process_extracted_reads(extract_file, region_str, motifs, ref_seq_list)
            visualize_data(reads_df)

            padded_reads = create_padded_reads(reads_df, regions_dict, region_length)
        else:

            reads_df, regions_dict = process_extracted_reads_no_fully_unmethylated(extract_file, region_str, motifs, ref_seq_list)
            visualize_data(reads_df)
            padded_reads = create_padded_reads_no_fully_unmethylated(reads_df, regions_dict, region_length)
        
        if padded_reads is not None:
            plot_padded_reads(padded_reads, ref_seq_list)
            save_padded_reads(padded_reads, output_dir, save_padded_reads_name_np)
    
    
if __name__ == "__main__":
    main()
